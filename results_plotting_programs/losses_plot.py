
"""Plots loss curves for various models"""

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

# original experiment 2
training_loss = [0.31835803562509163, 0.31747598286305945, 0.29990835771415286, 0.28791594749203986,
                 0.27881704604382496, 0.27236192094003325, 0.26594746536630115, 0.2603536015706442,
                 0.2555139650227614, 0.24804157002030738, 0.21527712123987178, 0.21095774049841945,
                 0.20874592998505262, 0.20629448373112724, 0.20442234172975474, 0.20188914531058375,
                 0.19923037503660762, 0.19695132523192108, 0.1940310521583472, 0.17666282877130526,
                 0.17323593791767714, 0.17242758525275967, 0.17097448258310627, 0.1703746804496263,
                 0.16884232682751568, 0.16740641692391112, 0.16623991885629238, 0.16612623006637608,
                 0.15484920516172476, 0.15238625387824248, 0.15189167089580027, 0.15109743002244327,
                 0.15106429445654176, 0.14998955653487167, 0.149029454759187, 0.1482300753814813,
                 0.14047274393461912, 0.14125930496834235, 0.13905506415112984, 0.13856250244140098,
                 0.13818602700339613, 0.13845576054571684, 0.13759445298749126, 0.1370485863527559]
validation_loss = [0.33481374605919073, 0.3013044333239186, 0.2937134558259239, 0.27152799371402697,
                   0.25731331051447764, 0.25457832131702934, 0.2627568236978644, 0.24244508856776753,
                   0.22894726517957273, 0.2232389952490336, 0.23886380205420205, 0.23158298087924817,
                   0.22345275305664072, 0.20532043443474496, 0.20377287958778897, 0.19612364798320114,
                   0.19549125013087387, 0.19035119322425886, 0.19100214012695726, 0.18983645910749267,
                   0.18702396109913252, 0.19653932279892133, 0.17598319551021005, 0.17604228713023204,
                   0.1702955554714234, 0.17272531099539032, 0.1685971474098285, 0.172578620804435,
                   0.1685304499909682, 0.1723806127912262, 0.1754641842258172, 0.15629421104903718,
                   0.16206531241699823, 0.1542955655678566, 0.15985602296368306, 0.1531589620775195,
                   0.16124438174783745, 0.15626027676686813, 0.1612576228518386, 0.16208062407999752,
                   0.1454262856672989, 0.1493538257884122, 0.14395756860727016, 0.14992820283117647]

training_loss_51 = [0.3249765573335545, 0.31628227659773855, 0.29898112401472304, 0.2864915023325614,
                    0.2777190035351959, 0.2711628670796447, 0.26547017118874, 0.25945587375166285,
                    0.25466727725434324, 0.2702965713106096, 0.21263636511530362, 0.20865807494140107,
                    0.20607487435906913, 0.203031802484587, 0.2009735514147825, 0.19842444038337523,
                    0.1960000322068851, 0.19411795388575798, 0.21608669137848274, 0.17499190466144893,
                    0.17343773791841194, 0.17279100859992144, 0.17127966509358855, 0.17089420449660686,
                    0.16933023069470024, 0.16802520339416718, 0.1670105303138185, 0.21779885851512,
                    0.15598594668072654, 0.15345477039847646, 0.15368970001314236, 0.1529094120933843,
                    0.15281739836841712, 0.15150584308702839, 0.15063807545325808, 0.14992738082596854,
                    0.16796238312963396, 0.14364597808775445, 0.14127142272677867, 0.14131251786246404,
                    0.14098864662103808, 0.14114971401917278, 0.14008938470247612, 0.13957263629727004,
                    0.13900534164035216, 0.1564484051902712, 0.13500119835006744, 0.13263721476402873,
                    0.1325850677137091, 0.13255496460530133, 0.13279615601962055, 0.13190330089602098,
                    0.13158800921988972, 0.13116039645124508, 0.14514152457039536, 0.12807805539271291,
                    0.12584784535920454, 0.12606776767720318, 0.126118170530581, 0.12634189815221683,
                    0.12553887105136247, 0.1252833949779874, 0.12506989625686485, 0.1390981799382384,
                    0.12252933018957365, 0.12071030840449298, 0.12075928173651818, 0.12079441369046885,
                    0.12106601949270783, 0.12028097491793217, 0.12012530777069763, 0.12002561552279316,
                    0.1309217671431335, 0.11762310039016713, 0.1160676246168357, 0.11615837540339946,
                    0.11618308966660201, 0.11651147324730979, 0.1158173612561806, 0.11573404290335354,
                    0.11558675897764747, 0.11602295247472025, 0.11283382665507807, 0.11171873141481907,
                    0.1118954142161987, 0.11185545374970812, 0.11223954447939927, 0.1115348485862489,
                    0.11143829728729517, 0.11140456783900887, 0.11119416940657954, 0.1090862551408165,
                    0.10814556415494933, 0.10808196212207459, 0.10801945996615707, 0.10835388828111057,
                    0.1076952518197709, 0.10761249888486209, 0.10760671884710964, 0.10209844427299686,
                    0.10566721378174862, 0.10453874872685254, 0.10454840046531609, 0.10441139147895723,
                    0.1046989218795066, 0.10414560351475197, 0.10409595145201656, 0.10423638932929973,
                    0.09604003841689389, 0.10258516068779729, 0.10153137233659879, 0.10163853088939023,
                    0.1014315978997286, 0.10161679437592541, 0.10099823789783466, 0.1008572071964022,
                    0.1011660647770932, 0.09249293680269537, 0.09910761905827664, 0.09827498791549308,
                    0.09840998941725096, 0.09833244683057041, 0.09850869331472725, 0.09797547234559631,
                    0.09786165531349218]
validation_loss_51 = [0.3382790102070076, 0.29969464147856856, 0.2798402107216841, 0.259383201810869,
                      0.2534289027798427, 0.2528191835813414, 0.2541967069228437, 0.2552360519717724,
                      0.22872146219157619, 0.2359019484690951, 0.22567383691095366, 0.21987075253162555,
                      0.22316093216926347, 0.19999316707696688, 0.20328656340796086, 0.19356300010226032,
                      0.20523191002596028, 0.1935550181505584, 0.19749241231632114, 0.19441948960439032,
                      0.1946367519888021, 0.18923518198309572, 0.17961265491743544, 0.18235022943744614,
                      0.1700942553169893, 0.18663127294246099, 0.16969807898179703, 0.18400444889245202,
                      0.17385223698114635, 0.1777215301502421, 0.17285776231611602, 0.16379206405673452,
                      0.16655261167969665, 0.15395218419202683, 0.17099265113365425, 0.1556114212936009,
                      0.16967504319684593, 0.1680135850029579, 0.16549512456506277, 0.1589511011975313,
                      0.1530138172222745, 0.15508734355265602, 0.14335484157049933, 0.16185548631367683,
                      0.14980005580674755, 0.15745026315050598, 0.16220291681383645, 0.15566348044549946,
                      0.14964606779832354, 0.14978428485279605, 0.14682415198560483, 0.1372487490388663,
                      0.15447177147474395, 0.14636885576022107, 0.14546892276533108, 0.15223139510825287,
                      0.15141156597905828, 0.14418165037345063, 0.14542416563312124, 0.14236544330145745,
                      0.13237553535284524, 0.15153598571150023, 0.14491149676053755, 0.14113018565257507,
                      0.14542078989965454, 0.1493027807105537, 0.13981184911845987, 0.13703564052201933,
                      0.13853941810542977, 0.12755673627567046, 0.15127593002952366, 0.1471425466823565,
                      0.13521452277204835, 0.14017338907660182, 0.14248331389016256, 0.13571919018568923,
                      0.13333327722546023, 0.13297194875023852, 0.12437017504270037, 0.1484817339779426,
                      0.1414880199045593, 0.1304933531529512, 0.1339576332673004, 0.1396417937191172,
                      0.1308535564671247, 0.12876094794027076, 0.13020333691337638, 0.12031701067056956,
                      0.1452194412861902, 0.13886747768538188, 0.1248619320404931, 0.12896851236765142,
                      0.13387575791917358, 0.12694517835286154, 0.12346451195450381, 0.12786921980498872,
                      0.11711132228838465, 0.1383112379079819, 0.1369244602424027, 0.12186207468766032,
                      0.12756990348084207, 0.12947229514762892, 0.12586496263248964, 0.12277025810767547,
                      0.12337813846157812, 0.11651990420320266, 0.13461711068783486, 0.13369434703870173,
                      0.11921976739007416, 0.12344522366126143, 0.12684771911293044, 0.12332880603339705,
                      0.11919163279593262, 0.12181296934469822, 0.1144296732341487, 0.12597654809057926,
                      0.12295992918566723, 0.11617933560772589, 0.12194600498324008, 0.12231093670424108,
                      0.12303886486590307, 0.11693214624271997, 0.11863474626151066, 0.11233587063189658,
                      0.12292675147412495]

# experiment 54: result with pYIN, three channels, same parameters as experiment 2
training_loss_54 = [0.33065208937558865, 0.3269819141133631, 0.2997947002960141, 0.2814391435380843,
                 0.26647618385761, 0.25556986872088716, 0.24559526613352098, 0.2381971778040879,
                 0.23262325663552016, 0.24039210917221177, 0.18812344812729967, 0.18341207488248606,
                 0.182346075082568, 0.18042144776786936, 0.17874076646760345, 0.17617987262188642,
                 0.1739936584261151, 0.17227957672084285, 0.17292215975208414, 0.15423263581742688,
                 0.15027173144556782, 0.1498316298070836, 0.14796879701678795, 0.14618496015706256,
                 0.14417582793470748, 0.14277131148260072, 0.14146806338007364, 0.17004099340798953,
                 0.12887404295897695, 0.1259460573843845]
validation_loss_54 = [0.3418056475021119, 0.31857388298123257, 0.2740889087139867, 0.24286184339850636,
                   0.23886442069165095, 0.21552194198807775, 0.2367907546134774, 0.20340922078822857,
                   0.2029879554364568, 0.22261527261161457, 0.21426544904981304, 0.19773805772439254,
                   0.1957098691185725, 0.19251980159305584, 0.18402975807861505, 0.20742714910715226,
                   0.1749560810472949, 0.1725111395776744, 0.17589486026669576, 0.1711834770108762,
                   0.16779579140852421, 0.15822059813399345, 0.15329427191527428, 0.15499105007827313,
                   0.16736470856284943, 0.14514201832241716, 0.14456204823592134, 0.17559485962565405,
                   0.13824005519453764, 0.14726077754441233]

# experiment 55: best result with two channels using MIDI, learning rate 0.000005
training_loss_55 = [0.3088651769129293, 0.33183028660436437, 0.3275832780515186, 0.3215774249763201,
                 0.31756379562679354, 0.3158976909066148, 0.315321681658539, 0.3130858064297631,
                 0.31075753831540237, 0.37800891205136267, 0.2946831655982318, 0.2901186632190296,
                 0.2868208295103063, 0.28486470559295574, 0.284897128869566, 0.2844965296976937,
                 0.2822970875496036, 0.2803230199536687, 0.334139324325536, 0.2650788637431467,
                 0.260371564030197, 0.2582735530956235, 0.25735284276384773, 0.257974933570948,
                 0.2581097973783317, 0.2567483577274447]
validation_loss_55 = [0.33437552841191887, 0.3295724422647561, 0.314981375991601, 0.302915188004259,
                   0.3109158160638258, 0.31201054591993616, 0.3225737703292946, 0.30921131208090025,
                   0.29379853047635673, 0.29382566124376625, 0.2896131083568814, 0.2910149466206472,
                   0.28182093967308786, 0.28056821338702015, 0.2778620730159295, 0.2790142748849896,
                   0.29304651107421736, 0.27266740292400965, 0.26723500332379724, 0.2632659356838169,
                   0.27433784158861907, 0.25876819540469564, 0.2573798226408621, 0.2629065589758393,
                   0.27341642820712597, 0.2724523750502787]

training_loss_55 = [0.3088651769129293, 0.33183028660436437, 0.3275832780515186, 0.3215774249763201,
                    0.31756379562679354, 0.3158976909066148, 0.315321681658539, 0.3130858064297631,
                    0.31075753831540237, 0.37800891205136267, 0.2946831655982318, 0.2901186632190296,
                    0.2868208295103063, 0.28486470559295574, 0.284897128869566, 0.2844965296976937,
                    0.2822970875496036, 0.2803230199536687, 0.334139324325536, 0.2650788637431467,
                    0.260371564030197, 0.2582735530956235, 0.25735284276384773, 0.257974933570948,
                    0.2581097973783317, 0.2567483577274447, 0.2555972315400808, 0.3027495052665472,
                    0.24746323280838523, 0.24295924187553228, 0.24181059012342632, 0.24140158276612111,
                    0.24245248907837666]
validation_loss_55 = [0.33437552841191887, 0.3295724422647561, 0.314981375991601, 0.302915188004259,
                      0.3109158160638258, 0.31201054591993616, 0.3225737703292946, 0.30921131208090025,
                      0.29379853047635673, 0.29382566124376625, 0.2896131083568814, 0.2910149466206472,
                      0.28182093967308786, 0.28056821338702015, 0.2778620730159295, 0.2790142748849896,
                      0.29304651107421736, 0.27266740292400965, 0.26723500332379724, 0.2632659356838169,
                      0.27433784158861907, 0.25876819540469564, 0.2573798226408621, 0.2629065589758393,
                      0.27341642820712597, 0.2724523750502787, 0.2571286032322998, 0.251174867543334,
                      0.25018830017564164, 0.26126188872204203, 0.24181059012342632, 0.24140158276612111,
                      0.24245248907837666]


plt.plot(training_loss, 'o', color='blue', label="training loss")
plt.plot(validation_loss, 'x', color='orange', label="validation loss")
plt.axvline(x=0, ls='dotted', label="epoch")
for i in [10, 19, 28, 36]:
    plt.axvline(x=i, ls='dotted')
ax = plt.axes()
ax.xaxis.label.set_size(13)
ax.yaxis.label.set_size(13)
ax.xaxis.set_major_locator(ticker.MultipleLocator(2000/466))
ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x * 466))
ax.xaxis.set_major_formatter(ticks_x)
ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:.2g}'.format(x))
ax.yaxis.set_major_formatter(ticks_y)
plt.xlabel("Training performances")
plt.ylabel("Mean squared error")
plt.legend(fontsize=13)
plt.savefig("/Users/scwager/Documents/autotune_fa18_data/plots/losses.eps", format="eps", bbox_inches='tight')
plt.show()

plt.plot(training_loss_51, '.', color='blue', label="training loss (3 ch.)")
plt.plot(validation_loss_51, 'x', color='orange', label="validation loss (3 ch.)")
plt.plot(training_loss_55, '*', color='green', label="training loss (2 ch.)")
plt.plot(validation_loss_55, '+', color='red', label="validation loss (2 ch.)")
plt.axvline(x=0, ls='dotted', label="epoch")
for i in [9, 18, 27, 36, 45, 54, 63, 72, 81, 90, 99, 108, 117]:
    plt.axvline(x=i, ls='dotted')
ax = plt.axes()
ax.xaxis.label.set_size(13)
ax.yaxis.label.set_size(13)
ax.xaxis.set_major_locator(ticker.MultipleLocator(10000/466))
ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x * 466))
ax.xaxis.set_major_formatter(ticks_x)
ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:.2g}'.format(x))
ax.yaxis.set_major_formatter(ticks_y)
plt.xlabel("Training performances")
plt.ylabel("Mean squared error")
plt.legend(fontsize=13)
plt.savefig("/Users/scwager/Documents/autotune_fa18_data/plots/losses_2_vs_3_ch.eps", format="eps", bbox_inches='tight')
plt.show()

plt.plot(training_loss_51, 'o', color='blue', label="training loss")
plt.plot(validation_loss_51, 'x', color='orange', label="validation loss")
plt.axvline(x=0, ls='dotted', label="epoch")
for i in [10, 19, 28, 36, 45, 54, 63, 72, 81, 90, 99, 108, 117]:
    plt.axvline(x=i, ls='dotted')
ax = plt.axes()
ax.xaxis.label.set_size(13)
ax.yaxis.label.set_size(13)
ax.xaxis.set_major_locator(ticker.MultipleLocator(10000/466))
ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x * 466))
ax.xaxis.set_major_formatter(ticks_x)
ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:.2g}'.format(x))
ax.yaxis.set_major_formatter(ticks_y)
plt.xlabel("Training performances")
plt.ylabel("Mean squared error")
plt.legend(fontsize=13)
plt.savefig("/Users/scwager/Documents/autotune_fa18_data/plots/losses_51.eps", format="eps", bbox_inches='tight')
plt.show()

plt.plot(training_loss_54, 'o', color='blue', label="training loss")
plt.plot(validation_loss_54, 'x', color='orange', label="validation loss")
plt.axvline(x=0, ls='dotted', label="epoch")
for i in [10, 19, 28, 36]:
    plt.axvline(x=i, ls='dotted')
ax = plt.axes()
ax.xaxis.label.set_size(13)
ax.yaxis.label.set_size(13)
ax.xaxis.set_major_locator(ticker.MultipleLocator(2000/466))
ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x * 466))
ax.xaxis.set_major_formatter(ticks_x)
ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:.2g}'.format(x))
ax.yaxis.set_major_formatter(ticks_y)
plt.xlabel("Training performances")
plt.ylabel("Mean squared error")
plt.legend(fontsize=13)
plt.savefig("/Users/scwager/Documents/autotune_fa18_data/plots/losses_54.eps", format="eps", bbox_inches='tight')
plt.show()

plt.plot(training_loss_55, 'o', color='blue', label="training loss")
plt.plot(validation_loss_55, 'x', color='orange', label="validation loss")
plt.axvline(x=0, ls='dotted', label="epoch")
for i in [10, 19, 28, 36]:
    plt.axvline(x=i, ls='dotted')
ax = plt.axes()
ax.xaxis.label.set_size(13)
ax.yaxis.label.set_size(13)
ax.xaxis.set_major_locator(ticker.MultipleLocator(2000/466))
ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x * 466))
ax.xaxis.set_major_formatter(ticks_x)
ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:.2g}'.format(x))
ax.yaxis.set_major_formatter(ticks_y)
plt.xlabel("Training performances")
plt.ylabel("Mean squared error")
plt.legend(fontsize=13)
plt.savefig("/Users/scwager/Documents/autotune_fa18_data/plots/losses_55.eps", format="eps", bbox_inches='tight')
plt.show()
